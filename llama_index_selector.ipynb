{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U git+https://github.com/santiagxf/llama_index.git@santiagxf/azure-ai-inference#subdirectory=llama-index-integrations/llms/llama-index-llms-azureai\n",
    "%pip install -U git+https://github.com/santiagxf/llama_index.git@santiagxf/azure-ai-inference#subdirectory=llama-index-integrations/embeddings/llama-index-embeddings-azureai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import ToolMetadata\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "from llama_index.llms.azureai.inference import AzureAIModelInference as AzureAIModelInferenceLLM\n",
    "from llama_index.embeddings.azureai.inference import AzureAIModelInference as AzureAIModelInferenceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureAIModelInferenceLLM(\n",
    "    endpoint=os.environ[\"AZURE_AI_COHERE_CMDR_ENDPOINT_URL\"],\n",
    "    credential=os.environ['AZURE_AI_COHERE_CMDR_ENDPOINT_KEY']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi3 = AzureAIModelInferenceLLM(\n",
    "    endpoint=os.environ[\"AZURE_AI_PHI3_MINI_ENDPOINT_URL\"],\n",
    "    credential=os.environ['AZURE_AI_PHI3_MINI_ENDPOINT_KEY']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = AzureAIModelInferenceEmbedding(\n",
    "    endpoint=os.environ[\"AZURE_AI_COHERE_EMBED_ENDPOINT_URL\"],\n",
    "    credential=os.environ['AZURE_AI_COHERE_EMBED_ENDPOINT_KEY'],\n",
    "    client_kwargs={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"data/paul_graham\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.chunk_size = 1024\n",
    "nodes = Settings.node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes, storage_context=storage_context)\n",
    "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "list_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=list_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to Paul Graham eassy on\"\n",
    "        \" What I Worked On.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from Paul Graham essay on What\"\n",
    "        \" I Worked On.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(llm=phi3),\n",
    "    query_engine_tools=[\n",
    "        list_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document is a collection of essays by Paul Graham, a computer programmer, entrepreneur, and investor, detailing his journey as a writer, programmer, and entrepreneur. Graham reflects on his early experiences with programming and writing, his education, and his evolving interests. He discusses his decision to pursue philosophy and AI in college, his graduate school experience, and his realization that the field of AI was a hoax, which led him to focus on Lisp. Graham also writes about his time at art school and his career at Interleaf, a software company. He founded Viaweb, an e-commerce software company, and later co-founded Y Combinator, a successful startup investment and mentoring program. Graham also worked on open-source projects, such as a new dialect of Lisp called Arc, and discovered the power of publishing essays online. He explores the impact of customs and traditions in various fields and shares his thoughts on independent thinking and rapid change. The essays offer insights into Graham's approach to work and life, showcasing his passion for programming, writing, and entrepreneurship.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After leaving RICS, Paul Graham returned to New York and resumed his previous life, but with the added benefit of financial security. He continued painting and experimented with new techniques, combining traditional painting with photography and printmaking. He also began searching for a new apartment to buy, contemplating which neighborhood to live in. During this time, he had an idea for a new startup, which led him to eventually found a new company called Aspra.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What did Paul Graham do after RICS?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
